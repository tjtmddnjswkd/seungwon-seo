{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tjtmddnjswkd/seungwon-seo/blob/master/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-EnI0K2waCw"
      },
      "source": [
        "import collections\n",
        "import logging\n",
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyXtUP6wx1WJ"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rkj7EeAEx-xq"
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhMshQRdyEZo"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-jdngUFyPlD"
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1qG_o4mzJdM"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead)\n",
        "  but it must be broadcastable for addition.\n",
        "\n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable\n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfg8DFLlx249"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "\n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7AXYDKFylaG"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGoezlroxJcR"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    return out2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmbbYWjKxVFJ"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
        "                                            self.d_model)\n",
        "\n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
        "                       for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "\n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqmXe6SHyyh0"
      },
      "source": [
        "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8,\n",
        "                         dff=2048, input_vocab_size=8500,\n",
        "                         maximum_position_encoding=10000)\n",
        "\n",
        "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-mCoJe2y5aD"
      },
      "source": [
        "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17T-Sn97TDQ3"
      },
      "source": [
        "############# 실제 사용할 코드 #######################"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXf8WI3GTFoc"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import (datasets, feature_extraction, linear_model, metrics)\n",
        "import pandas as pd\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-t8OYWoTITc"
      },
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q9H3gTkTJtS"
      },
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c18DMQqJUkpM",
        "outputId": "42160022-95e2-41ea-c3e9-86d494b00b9b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obz7i2ZlTKUV"
      },
      "source": [
        "order = '/1-35/'\n",
        "\n",
        "data_path = '/content/drive/Shareddrives/딥러닝팀플/data/paper'\n",
        "# data_file = os.path.join(data_path, '1-35.zip')\n",
        "\n",
        "# with zipfile.ZipFile(data_file, 'r') as file: # zip 파일 압축 해제\n",
        "#   file.extractall('%s/1-35' % data_path)\n",
        "\n",
        "file_list = os.listdir('%s/1-35' % data_path)\n",
        "\n",
        "data_path += order\n",
        "\n",
        "count = 0 \n",
        "for i in range(len(file_list)):\n",
        "    count += 1\n",
        "    if i == 0:\n",
        "        df = pd.read_excel('%s%s' % (data_path, file_list[i]))\n",
        "    df = df.append(pd.read_excel('%s%s' % (data_path, file_list[i])))\n",
        "\n",
        "df = df[['Abstract', 'WoS Categories']]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7BcK28zeXu9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebacd0c5-4d98-4049-fe95-469e5832136b"
      },
      "source": [
        "print(df.isnull().sum())\n",
        "print('데이터 개수 : %d' % df.shape[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Abstract          11177\n",
            "WoS Categories        0\n",
            "dtype: int64\n",
            "데이터 개수 : 35097\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2pke5ifeada"
      },
      "source": [
        "df.dropna(inplace=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "619UHeRtVCfE",
        "outputId": "820530be-dacd-46ad-bf5a-989c5ff6991b"
      },
      "source": [
        "print(df.isnull().sum())\n",
        "print('데이터 개수 : %d' % df.shape[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Abstract          0\n",
            "WoS Categories    0\n",
            "dtype: int64\n",
            "데이터 개수 : 23920\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPoGNqgVF1Lp"
      },
      "source": [
        "df.drop_duplicates(['Abstract'], inplace=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4cnZIlEGBZG",
        "outputId": "6c7bd763-7684-4681-b2df-e7230328a59b"
      },
      "source": [
        "print(df.isnull().sum())\n",
        "print('데이터 개수 : %d' % df.shape[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Abstract          0\n",
            "WoS Categories    0\n",
            "dtype: int64\n",
            "데이터 개수 : 21347\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgarYALIeeYH"
      },
      "source": [
        "category = []\n",
        "for i in df['WoS Categories']:\n",
        "  for j in i.split('; '):\n",
        "    category.append(j)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26g0AB15elEa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00c1160e-2f89-4f04-84e6-161c7011f05e"
      },
      "source": [
        "unique_category = set(category)\n",
        "print(unique_category)\n",
        "print(len(unique_category))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Biochemistry & Molecular Biology', 'Energy & Fuels', 'Materials Science, Multidisciplinary', 'Psychiatry', 'Immunology', 'Physics, Applied', 'Nanoscience & Nanotechnology', 'Mathematics, Interdisciplinary Applications', 'Business, Finance', 'Computer Science, Hardware & Architecture', 'Geography', 'Computer Science, Information Systems', 'Psychology, Experimental', 'Automation & Control Systems', 'Oncology', 'Mathematics', 'Mechanics', 'Zoology', 'International Relations', 'Computer Science, Artificial Intelligence', 'Chemistry, Multidisciplinary', 'Chemistry, Physical', 'Engineering, Chemical', 'Biotechnology & Applied Microbiology', 'Management', 'Medicine, General & Internal', 'Mathematics, Applied', 'Language & Linguistics', 'Telecommunications', 'Neurosciences', 'Education & Educational Research', 'Computer Science, Interdisciplinary Applications', 'Genetics & Heredity', 'Sociology', 'Economics', 'Engineering, Electrical & Electronic', 'Geosciences, Multidisciplinary', 'Communication', 'Ecology', 'Veterinary Sciences', 'Clinical Neurology', 'Psychology', 'Business', 'Evolutionary Biology', 'Psychology, Applied', 'Hospitality, Leisure, Sport & Tourism', 'Linguistics', 'Environmental Sciences', 'Environmental Studies', 'Physics, Mathematical', 'Fisheries', 'Surgery', 'Cell Biology', 'Public, Environmental & Occupational Health', 'Pharmacology & Pharmacy', 'Plant Sciences', 'Physics, Condensed Matter', 'Law', 'Political Science', 'Psychology, Multidisciplinary', 'Physics, Fluids & Plasmas'}\n",
            "61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWbYS8bBVLMG"
      },
      "source": [
        "def preprocessing(text):\n",
        "    pattern = '(\\[a-zA-Z0-9\\_.+-\\]+@\\[a-zA-Z0-9]+.\\[a-zA-Z0-9-.\\]+)' # email제거\n",
        "    text = re.sub(pattern=pattern,repl=' ',string=text)    \n",
        "\n",
        "    pattern = re.compile(r'([^\\w]?\\d+\\.?\\,?\\)?\\d*)+') # 숫자 제거\n",
        "    text = re.sub(pattern=pattern,repl=' ',string=text)\n",
        "    \n",
        "    pattern = '<[^>]*>' # html 태그 제거\n",
        "    text = re.sub(pattern=pattern,repl=' ',string=text)\n",
        "    \n",
        "    pattern = '[\\r|\\n]' # \\r,\\n 제거\n",
        "    text = re.sub(pattern=pattern,repl=' ',string=text)\n",
        "    \n",
        "    pattern= '[^\\w\\s]' # 특수기호 제거\n",
        "    text = re.sub(pattern=pattern,repl=' ',string=text)\n",
        "    \n",
        "    pattern=re.compile(r'\\s+')  #  이중 space 제거\n",
        "    text = re.sub(pattern=pattern,repl=' ',string=text)\n",
        "\n",
        "    return text"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLbIhhlMTdxf"
      },
      "source": [
        "df['Abstract'] = df['Abstract'].astype('string')\n",
        "df['Abstract'] = df['Abstract'].apply(preprocessing)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1R0a3zxTwsz",
        "outputId": "902e602f-4db1-4ab9-ce5b-eadfb6003a30"
      },
      "source": [
        "# Using Keras for word-level one-hot encoding\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(df['Abstract'])\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(df['Abstract'])\n",
        "\n",
        "one_hot_results = tokenizer.texts_to_matrix(df['Abstract'], mode='binary')\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 61904 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYrz-r8dTysV"
      },
      "source": [
        "maxlen = 0\n",
        "\n",
        "for i in sequences:\n",
        "  if maxlen < len(i):\n",
        "    maxlen = len(i)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1WzgnEWT0ET"
      },
      "source": [
        "x = sequences\n",
        "y = list(df['WoS Categories'])\n",
        "\n",
        "for i in range(len(y)):\n",
        "    label = y[i].split('; ')\n",
        "    y[i] = label\n",
        "label_dict = {}\n",
        "count = 0\n",
        "for i in y: # label 정수 인코딩을 위한 dictionary 생성\n",
        "    for j in i:\n",
        "        if j not in label_dict.keys():\n",
        "            label_dict[j] = count\n",
        "            count += 1 "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvbBHhFcT2G_"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2, random_state=1)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=.2, random_state=2)\n",
        "def vectorize_sequences(sequences, dimension=(len(label_dict.keys()))):\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  count = 0\n",
        "  for i in sequences:\n",
        "    for j in i:\n",
        "      results[count, label_dict[j]] = 1.\n",
        "    count += 1  \n",
        "  return results\n",
        "\n",
        "y_train = vectorize_sequences(y_train)\n",
        "y_val = vectorize_sequences(y_val)\n",
        "y_test = vectorize_sequences(y_test)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wipx3sMKT3vY"
      },
      "source": [
        "from tensorflow.keras import preprocessing\n",
        "\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_val = preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjkcMjXCVuNH",
        "outputId": "868ee0e7-9bbc-4144-cbcd-d74fd6962f42"
      },
      "source": [
        "maxlen"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1208"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LVZvUniT-f8"
      },
      "source": [
        "embed_dim = 100  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 200  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, 10000, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(100, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(len(label_dict.keys()), activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f02UC1hUBol",
        "outputId": "c5657dd6-a8d7-42cd-ccb3-9f86695c2be8"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "filepath = '/content/drive/My Drive/project/model.{epoch:02d}.h5'\n",
        "modelckpt = ModelCheckpoint(filepath=filepath)\n",
        "model.compile(\"adam\", \"binary_crossentropy\", metrics='acc')\n",
        "history = model.fit(\n",
        "    x_train, y_train, batch_size=128, epochs=30, validation_data=(x_val, y_val)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "107/107 [==============================] - 54s 463ms/step - loss: 0.3188 - acc: 0.0194 - val_loss: 0.1323 - val_acc: 0.0460\n",
            "Epoch 2/30\n",
            "107/107 [==============================] - 51s 478ms/step - loss: 0.1377 - acc: 0.0400 - val_loss: 0.1313 - val_acc: 0.0460\n",
            "Epoch 3/30\n",
            "107/107 [==============================] - 53s 496ms/step - loss: 0.1354 - acc: 0.0413 - val_loss: 0.1312 - val_acc: 0.0460\n",
            "Epoch 4/30\n",
            "107/107 [==============================] - 53s 492ms/step - loss: 0.1340 - acc: 0.0472 - val_loss: 0.1175 - val_acc: 0.1710\n",
            "Epoch 5/30\n",
            " 32/107 [=======>......................] - ETA: 34s - loss: 0.1173 - acc: 0.1183"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jjSfEygVeqF"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP7CSuU0KGbM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}